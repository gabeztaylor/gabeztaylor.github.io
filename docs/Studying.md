---
layout: page
title: Studying
permalink: /Studying/
---

### 2/15/26

- **7:30 AM – 8:00 AM**: Relearned comparative advantage
  - **Sources**
    - https://en.wikipedia.org/wiki/Comparative_advantage
  - **Notes**
    - So the only reason this works is because of relative opportunity cost. Even if party A has an absolute advantage in efficiency over party B, paty A has an opportunity cost in choosing to spend time making something that they aren't **most** efficient at. Thus, they leave it to the scrubs to make that thing (even thought party A is better at it).

- **8:00 AM – 8:30 AM**: Continued learning transformers
  - **Sources**
    - Karpathy's tutorial on youtube
  - **Notes**
    - Biggest takeaway from this session was learning the difference between encoder and decoder, which he doesn't really explain until the end


- **8:28 AM – 9:13 AM**: Built this app, made some git repos and pushed changes


- **9:18 AM – 12:42 PM**: Continued studying Asymptotics of RL paper
  - **Sources**
    - https://arxiv.org/pdf/1911.07304
  - **Notes**
    - The measure for the weight distribution is frozen in time, but the solution still evolves in time. Roughly, the network is so wide that we're making tiny enough updates that the distribution doesn't change, but the accumulated changes over all weights is $$O(1)$$. The learning is driven from the kernel $$A$$ and the TD-error given from the environment. The network acts as like a fixed feature space instead of learning to represent new features.


